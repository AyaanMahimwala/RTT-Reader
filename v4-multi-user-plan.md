# V4: Multi-User Telegram Bot Plan

**Goal**: Let another developer test the RTT-Reader agent on their own calendar data via the same Telegram bot instance, with minimal code changes.

**Target user**: A developer who can clone the repo and run `data-extract.py` locally with their own Google credentials.

---

## Architecture Overview

### Per-User Data Directories

Each registered user gets an isolated data directory under `{DATA_DIR}/users/{telegram_user_id}/`:

```
DATA_DIR/
├── users.json                          # Registry: telegram_user_id → {name, status, registered_at}
├── calendar.db                         # Admin's existing data (unchanged, backward compat)
├── calendar_vectors/                   # Admin's existing vectors
├── memory.json                         # Admin's existing memories
└── users/
    └── {telegram_user_id}/
        ├── calendar_raw_full.csv       # Uploaded by user
        ├── calendar.db                 # Generated by ETL
        ├── calendar_vectors/           # Generated by ETL
        ├── memory.json                 # Per-user memories
        ├── discovery_cache.json        # ETL cache (resume-safe)
        ├── enrichment_cache.json       # ETL cache (resume-safe)
        └── taxonomy.json               # Generated or copied from admin
```

### User Flow

```
1. Developer sends /register to bot
       ↓
2. Bot creates entry in users.json + data directory
       ↓
3. Developer runs data-extract.py locally on their laptop
   (with their own Google Calendar + service account credentials)
       ↓
4. Developer sends the calendar_raw_full.csv file to the bot via Telegram
       ↓
5. Bot saves CSV to their data directory
       ↓
6. Developer sends /process to trigger ETL
       ↓
7. Bot runs the 3-pass ETL pipeline on their CSV
   (discovery → enrichment → embedding)
       ↓
8. Developer can now query their calendar data normally
```

---

## File Changes

### 1. `telegram_bot.py` — Multi-user commands & routing

**Changes:**
- Replace single `TELEGRAM_USER_ID` auth with a registry-based system
- The original `TELEGRAM_USER_ID` becomes the **admin** user (auto-registered, has access to `/sync`)
- Add new commands and handlers:

```
/register  — Register as a new user, creates data directory
/status    — Show registration status, data stats (events count, etc.)
/process   — Run ETL on uploaded CSV
/start     — Updated help text showing all commands
/new       — Unchanged (reset session)
/sync      — Admin only (requires Google credentials on server)
```

- Add **file upload handler**: When a registered user sends a `.csv` file, save it to their data directory
- **Message handler**: Look up user's data_dir, pass it through to `run_agent()`
- **Authorization logic**:
  - Admin user (from `TELEGRAM_USER_ID` env var): auto-registered, full access
  - Registered users: can query, upload CSV, run `/process`
  - Unregistered users: see a message directing them to `/register`

**User registry** (`users.json`):
```json
{
  "123456789": {
    "name": "Ayaan",
    "status": "ready",
    "registered_at": "2026-02-24T10:00:00",
    "is_admin": true
  },
  "987654321": {
    "name": "TestDev",
    "status": "registered",
    "registered_at": "2026-02-24T12:00:00",
    "is_admin": false
  }
}
```

Status values: `registered` (no data yet) → `processing` (ETL running) → `ready` (can query)

### 2. `db.py` — Parameterize all data paths

**Current problem**: All paths are module-level globals (`DB_FILE`, `VECTOR_DIR`, `MEMORY_FILE`, `TAXONOMY_FILE`). Functions use these globals directly.

**Solution**: Add a `data_dir` parameter to every function that touches the filesystem. Keep the existing globals as defaults for backward compatibility.

**Specific changes:**
- `get_connection(data_dir=None)` — Use `data_dir/calendar.db` if provided, else fall back to global `DB_FILE`
- `_get_vector_table(data_dir=None)` — Per-user vector table lookup (can't use global singleton). Use a dict cache: `{data_dir: table}`
- `run_sql(query, max_rows=200, data_dir=None)`
- `get_schema(data_dir=None)`
- `get_sample_rows(n=10, data_dir=None)`
- `get_category_distribution(data_dir=None)`
- `get_people_frequency(data_dir=None)`
- `semantic_search(query, n=20, filters=None, data_dir=None)`
- `find_similar_events(event_id, n=10, data_dir=None)`
- `_load_memories(data_dir=None)` / `_save_memories(data, data_dir=None)`
- `save_memory(text, category, tags=None, data_dir=None)`
- `search_memories(query, data_dir=None)`
- `list_memories(category=None, data_dir=None)`
- `execute_tool(tool_name, tool_input, data_dir=None)` — Passes `data_dir` through to all called functions

### 3. `agent.py` — Per-user agent context

**Changes:**
- `run_agent(question, session_id=None, data_dir=None)` — Pass `data_dir` through to all `db.py` calls
- `_build_system_prompt(data_dir=None)` — Load schema and memories from user-specific paths
- `_get_schema_text(data_dir=None)` — Per-user schema cache (dict keyed by `data_dir` instead of single global)
- `_get_memory_prompt(data_dir=None)` — Load from user-specific `memory.json`
- `execute_tool()` calls get `data_dir` forwarded
- The dynamic system prompt (event counts, etc.) will naturally reflect each user's data since it queries their DB

### 4. `etl.py` — Accept configurable paths

**Changes:**
- Add `run_etl(data_dir)` function that runs the full pipeline against a specific data directory
- This function replaces `main()` for multi-user usage:
  1. Reads `{data_dir}/calendar_raw_full.csv`
  2. Runs Pass 1 (discovery) with cache at `{data_dir}/discovery_cache.json`
  3. Generates or copies taxonomy to `{data_dir}/taxonomy.json`
  4. Runs Pass 2 (enrichment) with cache at `{data_dir}/enrichment_cache.json`
  5. Creates `{data_dir}/calendar.db`
  6. Creates `{data_dir}/calendar_vectors/`
- For the taxonomy: copy the admin's `taxonomy.json` as a starting point (since calendar events tend to have similar categories). If the admin doesn't have one, generate fresh.
- All file paths derived from the `data_dir` parameter instead of module-level globals
- `main()` still works unchanged for backward compat (uses default `_DATA_DIR`)

### 5. `sync.py` — Minor change

- `sync_calendar(data_dir=None)` — Pass `data_dir` through to `etl` and `db` functions
- For v1: `/sync` remains admin-only since it requires Google credentials on the server
- Future: per-user Google OAuth could enable sync for all users

---

## What This Plan Does NOT Include (Deferred to v2)

- **No OAuth website** — Users run `data-extract.py` locally (they're developers, they have Python)
- **No per-user Google credentials on the server** — Only admin can `/sync`
- **No rate limiting or quotas** — Trusted developers only
- **No user management UI** — Admin manages via JSON file
- **No data encryption at rest** — Trusted environment
- **No parallel ETL** — One user processes at a time (ETL is CPU/API-bound anyway)
- **No web UI per-user** — Telegram-only for v1

---

## Migration Path

1. Existing admin data stays in the root `DATA_DIR` (no migration needed)
2. Admin user is auto-registered on first bot startup using `TELEGRAM_USER_ID`
3. When admin queries, `data_dir=None` falls through to existing globals (full backward compat)
4. New users get their own directory under `{DATA_DIR}/users/{telegram_user_id}/`

---

## Implementation Order

1. **`db.py`**: Add `data_dir` parameter to all functions (backward compatible — `None` means use globals)
2. **`agent.py`**: Thread `data_dir` through `run_agent` → system prompt → tool execution
3. **`etl.py`**: Add `run_etl(data_dir)` function
4. **`telegram_bot.py`**: User registry, `/register`, file upload, `/process`, `/status`, routing
5. **Test**: Register a second user, upload CSV, process, query

---

## New User Experience (Step-by-Step)

```
Developer                              Telegram Bot
────────                               ────────────
/register                    →
                             ←         "Welcome! You're registered.
                                        Next steps:
                                        1. Clone the repo
                                        2. Set up .env with your Google Calendar credentials
                                        3. Run: python data-extract.py
                                        4. Send me the calendar_raw_full.csv file
                                        5. Then send /process"

[Runs data-extract.py locally]

[Sends calendar_raw_full.csv] →
                             ←         "CSV received! 3,421 events found.
                                        Send /process to start the ETL pipeline."

/process                     →
                             ←         "Processing your calendar data...
                                        Pass 1/3: Discovering categories..."
                             ←         "Pass 2/3: Enriching events..."
                             ←         "Pass 3/3: Building vector embeddings..."
                             ←         "Done! Your data is ready.
                                        3,421 events | 5,892 sub-activities
                                        Ask me anything about your calendar!"

"Who do I spend the most    →
 time with?"
                             ←         "Based on your calendar data, you spend
                                        the most time with Sarah (142 events)..."
```

---

## Cost Estimate for New User ETL

Based on the existing pipeline's ~$16 API spend for ~8,200 events:
- **Pass 1 (Haiku)**: ~$0.30 per 1,000 events
- **Pass 2 (Haiku)**: ~$1.00 per 1,000 events
- **Pass 3 (OpenAI embeddings)**: ~$0.02 per 1,000 sub-activities
- **Taxonomy (Sonnet, one-time)**: ~$0.05

For a typical user with ~3,000 events: **~$4-5 total**.
